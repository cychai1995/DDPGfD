---
train_config:
  env_port: 19999
  seed: 1000
  exp_name: 's0'
  lr_rate: 0.0005
  w_decay: 0.00001
  restore: false # Restore for training or evaluation
  tps: 1800 # Restored episode for evaluation
  n_episode: 1800 # Total Training Episode
  batch_size: 4096 # Sample from PER
  update_step: 5 # Update per env step
  mse_loss: true # for Q loss
  save_every: 100 # save and evaluate
  # Pretrain Setting
  pretrain_demo: false
  pretrain_step: 10000
  pretrain_save_step: 2000

  eval_episode: 20
  train_gui: false
  eval_gui: true

agent_config:
  state_dim: 16
  action_dim: 4
  N_step: 10  # N step backup
  gamma: 0.9 # Reward discount
  seed: 54760
  replay_buffer_size: 100000
  discrete_update: false
  discrete_update_eps: 5 # Discrete update every N episode
  tau: 0.3 # Continuous update, source network portion, per episode
  action_noise_std: 0.1
  const_demo_priority: 0.99 # Should be less than 1
  const_min_priority: 0.001
  no_per: false # No prioritized experience replay (Don't  update priority)

demo_config:
  load_demo_data: false
  demo_dir: './data/demo'
  demo_N: 0 # manually fill in
  load_N: 0  # manually fill in
  prefix: 'demo_' # demo_i.pkl
